{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d27d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe2793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt embedding: map digit to string or word\n",
    "digit_to_string = {i: str(i) for i in range(10)}\n",
    "digit_to_word = {0: \"zero\", 1: \"one\", 2: \"two\", 3: \"three\", 4: \"four\", 5: \"five\", 6: \"six\", 7: \"seven\", 8: \"eight\", 9: \"nine\"}\n",
    "\n",
    "# Simple prompt encoder (embedding layer)\n",
    "class PromptEncoder(nn.Module):\n",
    "  def __init__(self, vocab, emb_dim=8, hidden_dim=32):\n",
    "    super(PromptEncoder, self).__init__()\n",
    "    self.vocab = vocab\n",
    "    self.emb = nn.Embedding(len(vocab), emb_dim)\n",
    "\n",
    "    self.to_gamma = nn.Linear(emb_dim, hidden_dim)\n",
    "    self.to_beta = nn.Linear(emb_dim, hidden_dim)\n",
    "\n",
    "  def forward(self, labels):\n",
    "    # labels is a list of strings (e.g., [\"zero\", \"one\", \"two\"])\n",
    "    idxs = torch.tensor([self.vocab[label] for label in labels])\n",
    "    idxs = idxs.to(next(self.parameters()).device)  # Move to same device as model\n",
    "    \n",
    "    emb = self.emb(idxs)\n",
    "\n",
    "    gamma = self.to_gamma(emb)\n",
    "    beta = self.to_beta(emb)\n",
    "\n",
    "    return gamma, beta \n",
    "  \n",
    "def apply_film(x, gamma, beta):\n",
    "  # x: [B, C, H, W], gamma/beta: [B, C]\n",
    "  # Reshape gamma/beta for broadcasting  \n",
    "  gamma = gamma[:, :, None, None]\n",
    "  beta = beta[:, :, None, None]\n",
    "  return gamma * x + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abf6af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple UNet for small images (28x28)\n",
    "class SmallUNet(torch.nn.Module):\n",
    "  def __init__(self, in_channels=1, out_channels=1, base_channels=16):  # Changed to 2 input channels\n",
    "    super(SmallUNet, self).__init__()\n",
    "    self.base_channels = base_channels\n",
    "\n",
    "    self.enc1 = nn.Sequential(nn.Conv2d(in_channels, base_channels, 3, padding=1), nn.SiLU())\n",
    "    self.enc2 = nn.Sequential(nn.Conv2d(base_channels, base_channels*2, 3, padding=1), nn.SiLU())\n",
    "    self.enc3 = nn.Sequential(nn.Conv2d(base_channels*2, base_channels*2, 3, padding=1), nn.SiLU())\n",
    "\n",
    "    self.dec3 = nn.Sequential(nn.Conv2d(base_channels*2, base_channels*2, 3, padding=1), nn.SiLU())\n",
    "    \n",
    "    self.up3 = nn.ConvTranspose2d(base_channels*2, base_channels*2, kernel_size=2, stride=2)\n",
    "    self.dec2 = nn.Sequential(nn.Conv2d(base_channels*4, base_channels, 3, padding=1), nn.SiLU())\n",
    "    \n",
    "    self.up2 = nn.ConvTranspose2d(base_channels, base_channels, kernel_size=2, stride=2)\n",
    "    self.dec1 = nn.Conv2d(base_channels*2, out_channels, 3, padding=1)\n",
    "\n",
    "    self.pool = nn.MaxPool2d(2)\n",
    "    self.up = nn.Upsample(scale_factor=2, mode='nearest') # ! ConvTranspose2d instead\n",
    "    \n",
    "    # timestep embedding\n",
    "    self.time_mlp = nn.Sequential(\n",
    "        nn.Linear(1, base_channels*2),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(base_channels*2, base_channels*2)\n",
    "    )\n",
    "\n",
    "  def forward(self, x, t, gamma, beta):\n",
    "    # t: [B] timesteps, scale to embedding\n",
    "    t = t[:, None].float() / 1000  # simple scaling\n",
    "    t_emb = self.time_mlp(t)[:, :, None, None]  # [B, hidden*2,1,1]\n",
    "    \n",
    "    # Encoder\n",
    "    e1 = self.enc1(x)\n",
    "    e1 = apply_film(e1, gamma[:, :self.base_channels], beta[:, :self.base_channels])\n",
    "\n",
    "    e2 = self.enc2(self.pool(e1))\n",
    "    e2 = apply_film(e2, gamma[:, :self.base_channels*2], beta[:, :self.base_channels*2])\n",
    "\n",
    "    e3 = self.enc3(self.pool(e2)) + t_emb\n",
    "    e3 = apply_film(e3, gamma[:, :self.base_channels*2], beta[:, :self.base_channels*2])\n",
    "    \n",
    "    # Decoder\n",
    "    d3 = self.dec3(e3)\n",
    "    d3 = apply_film(d3, gamma[:, :self.base_channels*2], beta[:, :self.base_channels*2])\n",
    "\n",
    "    d2 = self.dec2(torch.cat([self.up3(d3), e2], dim=1))\n",
    "    d2 = apply_film(d2, gamma[:, :self.base_channels], beta[:, :self.base_channels])\n",
    "\n",
    "    out = self.dec1(torch.cat([self.up2(d2), e1], dim=1))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a03aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdfa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocab for prompt encoder\n",
    "use_words = True  # Set False to use strings \"1\", \"2\", ...\n",
    "if use_words:\n",
    "  vocab_list = [digit_to_word[i] for i in range(10)]\n",
    "else:\n",
    "  vocab_list = [digit_to_string[i] for i in range(10)]\n",
    "vocab = {v: i for i, v in enumerate(vocab_list)}\n",
    "\n",
    "print(f\"Vocab: {vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aeb8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3️⃣ Diffusion utils\n",
    "# -------------------------\n",
    "T = 100  # number of diffusion steps\n",
    "beta = np.linspace(1e-4, 0.02, T)\n",
    "alpha = 1.0 - beta\n",
    "alpha_bar = np.cumprod(alpha)\n",
    "\n",
    "def q_sample(x0, t, noise=None):\n",
    "    \"\"\"Sample from q(x_t | x_0) - the forward diffusion process\"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x0)\n",
    "    \n",
    "    # Convert t to cpu for numpy indexing, then back to device\n",
    "    t_cpu = t.cpu()\n",
    "    a_bar = torch.tensor(alpha_bar[t_cpu], device=x0.device, dtype=x0.dtype)\n",
    "    \n",
    "    # Reshape for broadcasting\n",
    "    while len(a_bar.shape) < len(x0.shape):\n",
    "        a_bar = a_bar.unsqueeze(-1)\n",
    "    \n",
    "    return a_bar.sqrt() * x0 + (1 - a_bar).sqrt() * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, optimizer, loss\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "unet = SmallUNet().to(device)\n",
    "prompt_encoder = PromptEncoder(vocab).to(device)\n",
    "optimizer = optim.Adam(list(unet.parameters()) + list(prompt_encoder.parameters()), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51649bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"./logs/diffusion/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop - Diffusion model training\n",
    "# The model learns to predict the noise that was added to create noisy images\n",
    "for epoch in range(15):\n",
    "    unet.train()\n",
    "    prompt_encoder.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        # Move data to device\n",
    "        imgs = imgs.to(device)\n",
    "        \n",
    "        # Get prompt embeddings\n",
    "        prompts = [digit_to_word[l.item()] if use_words else digit_to_string[l.item()] for l in labels]\n",
    "        prompt_gamma, prompt_beta = prompt_encoder(prompts)\n",
    "        \n",
    "        # Sample random timesteps for each image in the batch\n",
    "        t = torch.randint(0, T, (imgs.size(0),), device=device)\n",
    "        \n",
    "        # Sample noise to add to the images\n",
    "        noise = torch.randn_like(imgs)\n",
    "        \n",
    "        # Create noisy images at timestep t using the forward diffusion process\n",
    "        x_t = q_sample(imgs, t, noise=noise)\n",
    "    \n",
    "        # Forward pass: model predicts the noise that was added\n",
    "        predicted_noise = unet(x_t, t, prompt_gamma, prompt_beta)\n",
    "        \n",
    "        # Loss: compare predicted noise with actual noise\n",
    "        loss = loss_fn(predicted_noise, noise)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    writer.add_scalar(\"Loss/Train\", avg_loss, epoch+1)\n",
    "    print(f\"Epoch {epoch+1} Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3547b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new image from text prompt - DDPM sampling\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_image_ddpm(prompt_text, model, prompt_encoder, device, num_steps=50):\n",
    "    \"\"\"Generate image using DDPM reverse process\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Start with pure noise\n",
    "        img = torch.randn(1, 1, 28, 28).to(device)\n",
    "        \n",
    "        # Get prompt embedding\n",
    "        prompt_gamma, prompt_beta = prompt_encoder([prompt_text])\n",
    "        \n",
    "        # Reverse diffusion process\n",
    "        timesteps = np.linspace(T-1, 0, num_steps).astype(int)\n",
    "        \n",
    "        for i, t_val in enumerate(timesteps):\n",
    "            t = torch.tensor([t_val], device=device).long()\n",
    "                        \n",
    "            # Predict noise\n",
    "            predicted_noise = model(img, t, prompt_gamma, prompt_beta)\n",
    "            \n",
    "            # Remove predicted noise (simplified DDPM step)\n",
    "            alpha_t = alpha[t_val]\n",
    "            alpha_bar_t = alpha_bar[t_val]\n",
    "            \n",
    "            if t_val > 0:\n",
    "                # Not the final step - add some randomness\n",
    "                beta_t = beta[t_val]\n",
    "                noise = torch.randn_like(img)\n",
    "                img = (1 / np.sqrt(alpha_t)) * (img - ((1 - alpha_t) / np.sqrt(1 - alpha_bar_t)) * predicted_noise)\n",
    "                img = img + np.sqrt(beta_t) * noise\n",
    "            else:\n",
    "                # Final step - no noise\n",
    "                img = (1 / np.sqrt(alpha_t)) * (img - ((1 - alpha_t) / np.sqrt(1 - alpha_bar_t)) * predicted_noise)\n",
    "        \n",
    "        # Clamp values\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "        \n",
    "        # Convert to numpy for display\n",
    "        generated_img = img.cpu().squeeze().numpy()\n",
    "        return generated_img\n",
    "\n",
    "def generate_image_simple(prompt_text, model, prompt_encoder, device):\n",
    "    \"\"\"Simple single-step generation (for comparison)\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Start with moderate noise\n",
    "        img = torch.randn(1, 1, 28, 28).to(device) * 0.5 + 0.5\n",
    "        \n",
    "        # Get prompt embedding\n",
    "        prompt_gamma, prompt_beta = prompt_encoder([prompt_text])\n",
    "                \n",
    "        # Use a moderate timestep\n",
    "        t = torch.tensor([T//2], device=device).long()\n",
    "        \n",
    "        # Predict noise and subtract it\n",
    "        predicted_noise = model(img, t, prompt_gamma, prompt_beta)\n",
    "        generated = img - predicted_noise * 0.3  # Scale factor for single step\n",
    "        generated = torch.clamp(generated, 0, 1)\n",
    "        \n",
    "        # Convert to numpy for display\n",
    "        generated_img = generated.cpu().squeeze().numpy()\n",
    "        return generated_img\n",
    "\n",
    "# Test generation after training (uncomment after training completes)\n",
    "# Generate and display images for different prompts\n",
    "prompts_to_test = [\"one\", \"two\", \"three\", \"four\", \"five\"]\n",
    "\n",
    "print(\"Ready to generate images. Run training first, then uncomment the generation code below.\")\n",
    "\n",
    "# Uncomment after training:\n",
    "\n",
    "fig, axes = plt.subplots(2, len(prompts_to_test), figsize=(15, 6))\n",
    "fig.suptitle('Image Generation: DDPM (top) vs Simple (bottom)')\n",
    "\n",
    "for i, prompt in enumerate(prompts_to_test):\n",
    "    # DDPM generation\n",
    "    generated_img_ddpm = generate_image_ddpm(prompt, unet, prompt_encoder, device)\n",
    "    axes[0, i].imshow(generated_img_ddpm, cmap='gray')\n",
    "    axes[0, i].set_title(f'DDPM: \"{prompt}\"')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Simple generation\n",
    "    generated_img_simple = generate_image_simple(prompt, unet, prompt_encoder, device)\n",
    "    axes[1, i].imshow(generated_img_simple, cmap='gray')\n",
    "    axes[1, i].set_title(f'Simple: \"{prompt}\"')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
